  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:00<00:08,  1.09it/s] 20%|██        | 2/10 [00:01<00:06,  1.26it/s] 30%|███       | 3/10 [00:02<00:06,  1.15it/s] 40%|████      | 4/10 [00:03<00:05,  1.12it/s] 50%|█████     | 5/10 [00:04<00:04,  1.11it/s] 60%|██████    | 6/10 [00:05<00:03,  1.08it/s] 70%|███████   | 7/10 [00:06<00:02,  1.06it/s] 80%|████████  | 8/10 [00:07<00:01,  1.10it/s] 90%|█████████ | 9/10 [00:08<00:00,  1.07it/s]100%|██████████| 10/10 [00:09<00:00,  1.07it/s]100%|██████████| 10/10 [00:09<00:00,  1.09it/s]
/home/mschlichting/RLSoups/average_models.py:70: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  weights = torch.nn.functional.softmax(performance)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.07s/it]100%|██████████| 1/1 [00:01<00:00,  1.07s/it]
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:01<00:12,  1.42s/it] 20%|██        | 2/10 [00:02<00:10,  1.36s/it] 30%|███       | 3/10 [00:04<00:09,  1.32s/it] 40%|████      | 4/10 [00:05<00:07,  1.28s/it] 50%|█████     | 5/10 [00:06<00:06,  1.30s/it] 60%|██████    | 6/10 [00:08<00:05,  1.35s/it] 70%|███████   | 7/10 [00:09<00:04,  1.43s/it] 80%|████████  | 8/10 [00:11<00:02,  1.43s/it] 90%|█████████ | 9/10 [00:12<00:01,  1.39s/it]100%|██████████| 10/10 [00:13<00:00,  1.33s/it]100%|██████████| 10/10 [00:13<00:00,  1.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.34s/it]100%|██████████| 1/1 [00:01<00:00,  1.34s/it]
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:01<00:14,  1.57s/it] 20%|██        | 2/10 [00:03<00:15,  1.88s/it] 30%|███       | 3/10 [00:05<00:11,  1.64s/it] 40%|████      | 4/10 [00:06<00:09,  1.52s/it] 50%|█████     | 5/10 [00:07<00:07,  1.47s/it] 60%|██████    | 6/10 [00:09<00:05,  1.43s/it] 70%|███████   | 7/10 [00:10<00:04,  1.46s/it] 80%|████████  | 8/10 [00:11<00:02,  1.43s/it] 90%|█████████ | 9/10 [00:13<00:01,  1.43s/it]100%|██████████| 10/10 [00:14<00:00,  1.45s/it]100%|██████████| 10/10 [00:14<00:00,  1.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.11s/it]100%|██████████| 1/1 [00:01<00:00,  1.11s/it]
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:01<00:10,  1.16s/it] 20%|██        | 2/10 [00:02<00:09,  1.20s/it] 30%|███       | 3/10 [00:03<00:08,  1.21s/it] 40%|████      | 4/10 [00:04<00:06,  1.14s/it] 50%|█████     | 5/10 [00:05<00:05,  1.08s/it] 60%|██████    | 6/10 [00:06<00:04,  1.08s/it] 70%|███████   | 7/10 [00:07<00:03,  1.11s/it] 80%|████████  | 8/10 [00:09<00:02,  1.14s/it] 90%|█████████ | 9/10 [00:10<00:01,  1.33s/it]100%|██████████| 10/10 [00:12<00:00,  1.33s/it]100%|██████████| 10/10 [00:12<00:00,  1.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.64s/it]100%|██████████| 1/1 [00:01<00:00,  1.64s/it]
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:00<00:08,  1.01it/s] 20%|██        | 2/10 [00:01<00:07,  1.03it/s] 30%|███       | 3/10 [00:03<00:07,  1.10s/it] 40%|████      | 4/10 [00:04<00:07,  1.18s/it] 50%|█████     | 5/10 [00:05<00:05,  1.15s/it] 60%|██████    | 6/10 [00:06<00:04,  1.14s/it] 70%|███████   | 7/10 [00:07<00:03,  1.14s/it] 80%|████████  | 8/10 [00:09<00:02,  1.22s/it] 90%|█████████ | 9/10 [00:10<00:01,  1.29s/it]100%|██████████| 10/10 [00:12<00:00,  1.29s/it]100%|██████████| 10/10 [00:12<00:00,  1.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.39s/it]100%|██████████| 1/1 [00:01<00:00,  1.39s/it]
0 0
Current Training Reward 48.34031134000001
0 1
Current Training Reward 67.22312194
0 2
Current Training Reward 75.54903422000001
0 3
Current Training Reward 61.768328190000005
0 4
Current Training Reward 72.18997781
0 5
Current Training Reward 44.339433400000004
0 6
Current Training Reward 57.49378781
0 7
Current Training Reward 48.03207644
0 8
Current Training Reward 66.48492040000001
0 9
Current Training Reward 61.474771219999994
[160.69295015965733, 61.628245666488226, 115.64429363691552, 152.38109148943252, 69.88393516854183, 124.69577749084544, 146.88351569403, 125.47821881147543, 154.30820755892674, 113.65397702263408]
[48.34031134000001, 67.22312194, 75.54903422000001, 61.768328190000005, 72.18997781, 44.339433400000004, 57.49378781, 48.03207644, 66.48492040000001, 61.474771219999994]
tensor([1.4737e-12, 2.3394e-04, 9.6606e-01, 1.0003e-06, 3.3588e-02, 2.6968e-14,
        1.3922e-08, 1.0828e-12, 1.1182e-04, 7.4581e-07])
all weights top-performance [129.36039264207392]
mean [160.69295015965733, 61.628245666488226, 115.64429363691552, 152.38109148943252, 69.88393516854183, 124.69577749084544, 146.88351569403, 125.47821881147543, 154.30820755892674, 113.65397702263408]
1 0
Current Training Reward 197.26901470103093
1 1
Current Training Reward 198.64266557
1 2
Current Training Reward 191.68233511999998
1 3
Current Training Reward 167.33195411
1 4
Current Training Reward 169.01767890999997
1 5
Current Training Reward 186.18038552000002
1 6
Current Training Reward 178.69956666
1 7
Current Training Reward 184.51893564835166
1 8
Current Training Reward 184.57474982000002
1 9
Current Training Reward 190.44188595
[282.98683753445164, 228.57834465117648, 238.98020446599293, 169.2663431328405, 295.11859175651546, 294.41714194447405, 239.72392952542856, 242.28792644753474, 264.9801451805689, 298.8612666590421]
[197.26901470103093, 198.64266557, 191.68233511999998, 167.33195411, 169.01767890999997, 186.18038552000002, 178.69956666, 184.51893564835166, 184.57474982000002, 190.44188595]
tensor([2.0183e-01, 7.9719e-01, 7.5636e-04, 2.0114e-14, 1.0854e-13, 3.0850e-06,
        1.7393e-09, 5.8574e-07, 6.1936e-07, 2.1878e-04])
all weights top-performance [202.09955660626545]
mean [282.98683753445164, 228.57834465117648, 238.98020446599293, 169.2663431328405, 295.11859175651546, 294.41714194447405, 239.72392952542856, 242.28792644753474, 264.9801451805689, 298.8612666590421]
2 0
Current Training Reward 306.4207616235294
2 1
Current Training Reward 277.69586818072287
2 2
Current Training Reward 278.59561884269664
2 3
Current Training Reward 292.39792963218395
2 4
Current Training Reward 307.85616341666673
2 5
Current Training Reward 320.18807380246915
2 6
Current Training Reward 273.313010172043
2 7
Current Training Reward 294.5979484666667
2 8
Current Training Reward 276.5464128720931
2 9
Current Training Reward 309.40559526436783
[340.9148407617284, 391.2013620203985, 279.0464863037431, 227.99634952022683, 334.31824351069963, 326.83395280989697, 316.70318680697727, 276.88218440956524, 303.80539064218783, 307.83396296181394]
[306.4207616235294, 277.69586818072287, 278.59561884269664, 292.39792963218395, 307.85616341666673, 320.18807380246915, 273.313010172043, 294.5979484666667, 276.5464128720931, 309.40559526436783]
tensor([1.0494e-06, 3.5144e-19, 8.6419e-19, 8.5285e-13, 4.4087e-06, 9.9997e-01,
        4.3894e-21, 7.6974e-12, 1.1134e-19, 2.0760e-05])
all weights top-performance [311.13686773734355]
mean [340.9148407617284, 391.2013620203985, 279.0464863037431, 227.99634952022683, 334.31824351069963, 326.83395280989697, 316.70318680697727, 276.88218440956524, 303.80539064218783, 307.83396296181394]
3 0
Current Training Reward 287.0669669791667
3 1
Current Training Reward 288.34845316
3 2
Current Training Reward 304.2374350978261
3 3
Current Training Reward 310.65924104444446
3 4
Current Training Reward 313.9524163157895
3 5
Current Training Reward 297.02923520408166
3 6
Current Training Reward 289.32118925
3 7
Current Training Reward 318.8844062022472
3 8
Current Training Reward 317.70090350574714
3 9
Current Training Reward 298.2409418842105
[298.44744039289264, 311.50628206757517, 326.24781559271406, 298.8296101369368, 255.6200164049431, 287.8853156629243, 299.9254114276583, 314.33082528671093, 355.0906715460849, 287.55405675200905]
[287.0669669791667, 288.34845316, 304.2374350978261, 310.65924104444446, 313.9524163157895, 297.02923520408166, 289.32118925, 318.8844062022472, 317.70090350574714, 298.2409418842105]
tensor([1.1571e-14, 4.1679e-14, 3.3144e-07, 2.0388e-04, 5.4901e-03, 2.4543e-10,
        1.1025e-13, 7.6122e-01, 2.3309e-01, 8.2445e-10])
all weights top-performance [376.2431627988138]
mean [298.44744039289264, 311.50628206757517, 326.24781559271406, 298.8296101369368, 255.6200164049431, 287.8853156629243, 299.9254114276583, 314.33082528671093, 355.0906715460849, 287.55405675200905]
4 0
Current Training Reward 299.99489808000004
4 1
Current Training Reward 280.61549478999996
4 2
Current Training Reward 309.7475380102041
4 3
Current Training Reward 307.42374799
4 4
Current Training Reward 317.8434764456522
4 5
Current Training Reward 330.51724070454543
4 6
Current Training Reward 300.9502264
4 7
Current Training Reward 301.23848411
4 8
Current Training Reward 280.77604063
4 9
Current Training Reward 317.20609314141416
[295.61562737347754, 286.94355703530005, 294.13036865658603, 351.3293399944405, 275.5998712248581, 306.9934981034115, 324.9021726313598, 358.4105520459001, 375.3871998670671, 320.3466457564392]
[299.99489808000004, 280.61549478999996, 309.7475380102041, 307.42374799, 317.8434764456522, 330.51724070454543, 300.9502264, 301.23848411, 280.77604063, 317.20609314141416]
tensor([5.5503e-14, 2.1279e-22, 9.5461e-10, 9.3458e-11, 3.1322e-06, 1.0000e+00,
        1.4428e-13, 1.9249e-13, 2.4984e-22, 1.6559e-06])
all weights top-performance [367.43700628401683]
mean [295.61562737347754, 286.94355703530005, 294.13036865658603, 351.3293399944405, 275.5998712248581, 306.9934981034115, 324.9021726313598, 358.4105520459001, 375.3871998670671, 320.3466457564392]
