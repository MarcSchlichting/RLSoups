  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:00<00:05,  1.80it/s] 20%|██        | 2/10 [00:00<00:02,  3.40it/s] 30%|███       | 3/10 [00:01<00:02,  2.78it/s] 40%|████      | 4/10 [00:01<00:02,  2.33it/s] 50%|█████     | 5/10 [00:02<00:02,  1.96it/s] 60%|██████    | 6/10 [00:02<00:01,  2.03it/s] 70%|███████   | 7/10 [00:03<00:01,  1.66it/s] 80%|████████  | 8/10 [00:04<00:01,  1.82it/s] 90%|█████████ | 9/10 [00:04<00:00,  1.99it/s]100%|██████████| 10/10 [00:04<00:00,  2.11it/s]100%|██████████| 10/10 [00:04<00:00,  2.07it/s]
/home/mschlichting/RLSoups/average_models.py:70: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  weights = torch.nn.functional.softmax(performance)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.80it/s]100%|██████████| 1/1 [00:00<00:00,  1.79it/s]
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:00<00:05,  1.72it/s] 20%|██        | 2/10 [00:01<00:04,  1.73it/s] 30%|███       | 3/10 [00:01<00:03,  1.81it/s] 40%|████      | 4/10 [00:02<00:03,  1.86it/s] 50%|█████     | 5/10 [00:02<00:02,  1.78it/s] 60%|██████    | 6/10 [00:03<00:02,  1.80it/s] 70%|███████   | 7/10 [00:03<00:01,  1.81it/s] 80%|████████  | 8/10 [00:04<00:01,  1.83it/s] 90%|█████████ | 9/10 [00:04<00:00,  1.83it/s]100%|██████████| 10/10 [00:05<00:00,  1.81it/s]100%|██████████| 10/10 [00:05<00:00,  1.81it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.88it/s]100%|██████████| 1/1 [00:00<00:00,  1.88it/s]
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:00<00:05,  1.70it/s] 20%|██        | 2/10 [00:01<00:04,  1.74it/s] 30%|███       | 3/10 [00:01<00:03,  1.80it/s] 40%|████      | 4/10 [00:02<00:03,  1.78it/s] 50%|█████     | 5/10 [00:03<00:03,  1.37it/s] 60%|██████    | 6/10 [00:03<00:02,  1.65it/s] 70%|███████   | 7/10 [00:04<00:01,  1.77it/s] 80%|████████  | 8/10 [00:04<00:01,  1.82it/s] 90%|█████████ | 9/10 [00:05<00:00,  1.83it/s]100%|██████████| 10/10 [00:05<00:00,  1.82it/s]100%|██████████| 10/10 [00:05<00:00,  1.74it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.85it/s]100%|██████████| 1/1 [00:00<00:00,  1.85it/s]
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:00<00:06,  1.34it/s] 20%|██        | 2/10 [00:01<00:05,  1.41it/s] 30%|███       | 3/10 [00:02<00:04,  1.50it/s] 40%|████      | 4/10 [00:02<00:04,  1.47it/s] 50%|█████     | 5/10 [00:03<00:03,  1.54it/s] 60%|██████    | 6/10 [00:03<00:02,  1.57it/s] 70%|███████   | 7/10 [00:04<00:01,  1.60it/s] 80%|████████  | 8/10 [00:05<00:01,  1.64it/s] 90%|█████████ | 9/10 [00:05<00:00,  1.63it/s]100%|██████████| 10/10 [00:06<00:00,  1.70it/s]100%|██████████| 10/10 [00:06<00:00,  1.59it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.70it/s]100%|██████████| 1/1 [00:00<00:00,  1.70it/s]
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:00<00:06,  1.42it/s] 20%|██        | 2/10 [00:03<00:16,  2.03s/it] 30%|███       | 3/10 [00:04<00:10,  1.44s/it] 40%|████      | 4/10 [00:05<00:07,  1.23s/it] 50%|█████     | 5/10 [00:06<00:05,  1.14s/it] 60%|██████    | 6/10 [00:07<00:04,  1.19s/it] 70%|███████   | 7/10 [00:08<00:03,  1.07s/it] 80%|████████  | 8/10 [00:09<00:02,  1.13s/it] 90%|█████████ | 9/10 [00:10<00:00,  1.01it/s]100%|██████████| 10/10 [00:13<00:00,  1.58s/it]100%|██████████| 10/10 [00:13<00:00,  1.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.37it/s]100%|██████████| 1/1 [00:00<00:00,  1.37it/s]
0 0
Current Training Reward 253.19929242000006
0 1
Current Training Reward 39.152508600000004
0 2
Current Training Reward 173.89765989000003
0 3
Current Training Reward 202.63261597000002
0 4
Current Training Reward 151.13123518000003
0 5
Current Training Reward 182.81132863
0 6
Current Training Reward 185.98230568999995
0 7
Current Training Reward 208.05738407
0 8
Current Training Reward 167.58731185999997
0 9
Current Training Reward 161.1648729
[266.9337876991161, 38.83378248693332, 179.8431407841733, 265.0530113542637, 283.5453553559805, 216.0990702381376, 306.23741330644543, 226.52590296919394, 167.80273147574604, 167.50940069793663]
[253.19929242000006, 39.152508600000004, 173.89765989000003, 202.63261597000002, 151.13123518000003, 182.81132863, 185.98230568999995, 208.05738407, 167.58731185999997, 161.1648729]
tensor([1.0000e+00, 0.0000e+00, 3.6286e-35, 1.0944e-22, 4.2039e-45, 2.6971e-31,
        6.4273e-30, 2.4838e-20, 6.5946e-38, 1.0714e-40])
all weights top-performance [268.51959308621554]
mean [266.9337876991161, 38.83378248693332, 179.8431407841733, 265.0530113542637, 283.5453553559805, 216.0990702381376, 306.23741330644543, 226.52590296919394, 167.80273147574604, 167.50940069793663]
1 0
Current Training Reward 260.59784081000004
1 1
Current Training Reward 253.27776376000006
1 2
Current Training Reward 274.36988917
1 3
Current Training Reward 249.31908345
1 4
Current Training Reward 273.59950324
1 5
Current Training Reward 271.07802300000003
1 6
Current Training Reward 264.19001988
1 7
Current Training Reward 269.40288819
1 8
Current Training Reward 263.76226998
1 9
Current Training Reward 272.24095431
[268.5761322972595, 261.66768843233064, 254.88404806758862, 256.24791590271377, 288.06518543095564, 269.6436263946957, 270.7005024107257, 264.78013944229053, 274.0383467071066, 291.50402313133253]
[260.59784081000004, 253.27776376000006, 274.36988917, 249.31908345, 273.59950324, 271.07802300000003, 264.19001988, 269.40288819, 263.76226998, 272.24095431]
tensor([6.4231e-07, 4.2528e-10, 6.1500e-01, 8.1178e-12, 2.8464e-01, 2.2868e-02,
        2.3325e-05, 4.2828e-03, 1.5207e-05, 7.3163e-02])
all weights top-performance [264.29558078200694]
mean [268.5761322972595, 261.66768843233064, 254.88404806758862, 256.24791590271377, 288.06518543095564, 269.6436263946957, 270.7005024107257, 264.78013944229053, 274.0383467071066, 291.50402313133253]
2 0
Current Training Reward 282.41253752
2 1
Current Training Reward 292.64324056
2 2
Current Training Reward 256.06436893
2 3
Current Training Reward 271.23166638000004
2 4
Current Training Reward 141.58677265999998
2 5
Current Training Reward 251.84041852
2 6
Current Training Reward 277.34405065999994
2 7
Current Training Reward 266.39248492999997
2 8
Current Training Reward 241.86114604000002
2 9
Current Training Reward 265.2431713
[283.95889606404, 257.9758332534477, 262.09389803451575, 288.93181658301893, 344.52410465200126, 109.46362285187976, 229.67911349618748, 251.39630180068133, 268.59083917971634, 279.4690039621143]
[282.41253752, 292.64324056, 256.06436893, 271.23166638000004, 141.58677265999998, 251.84041852, 277.34405065999994, 266.39248492999997, 241.86114604000002, 265.2431713]
tensor([3.6045e-05, 9.9996e-01, 1.3001e-16, 5.0240e-10, 0.0000e+00, 1.9035e-18,
        2.2679e-07, 3.9758e-12, 8.8226e-23, 1.2597e-12])
all weights top-performance [262.9655820285075]
mean [283.95889606404, 257.9758332534477, 262.09389803451575, 288.93181658301893, 344.52410465200126, 109.46362285187976, 229.67911349618748, 251.39630180068133, 268.59083917971634, 279.4690039621143]
3 0
Current Training Reward 267.00853523
3 1
Current Training Reward 318.65442543
3 2
Current Training Reward 302.06413779999997
3 3
Current Training Reward 298.65689156
3 4
Current Training Reward 291.38831416
3 5
Current Training Reward 299.42039495999995
3 6
Current Training Reward 337.76451645000003
3 7
Current Training Reward 284.42171316
3 8
Current Training Reward 290.67442316
3 9
Current Training Reward 253.32055497999997
[315.4328377593756, 393.696430952746, 330.43286246323953, 341.1264994210501, 321.55931511943083, 307.3941768361677, 308.7565870079822, 292.67867144089496, 309.2853443695068, 297.8432987852669]
[267.00853523, 318.65442543, 302.06413779999997, 298.65689156, 291.38831416, 299.42039495999995, 337.76451645000003, 284.42171316, 290.67442316, 253.32055497999997]
tensor([1.8667e-31, 5.0186e-09, 3.1298e-16, 1.0370e-17, 7.2288e-21, 2.2252e-17,
        1.0000e+00, 6.8158e-24, 3.5401e-21, 2.1206e-37])
all weights top-performance [306.58133102273257]
mean [315.4328377593756, 393.696430952746, 330.43286246323953, 341.1264994210501, 321.55931511943083, 307.3941768361677, 308.7565870079822, 292.67867144089496, 309.2853443695068, 297.8432987852669]
4 0
Current Training Reward 367.20946928999996
4 1
Current Training Reward 443.4549733399999
4 2
Current Training Reward 557.6300092300002
4 3
Current Training Reward 502.76846610999996
4 4
Current Training Reward 478.34291271000006
4 5
Current Training Reward 509.44765577
4 6
Current Training Reward 422.7316338499999
4 7
Current Training Reward 433.61201734
4 8
Current Training Reward 413.71486029000005
4 9
Current Training Reward 498.67011199
[301.5179089176025, 1740.9827656606553, 415.8845101466536, 631.0890269126093, 613.2012066049707, 905.1287841259718, 402.7346400376497, 827.3557416323247, 353.3068394399494, 1444.0416896968084]
[367.20946928999996, 443.4549733399999, 557.6300092300002, 502.76846610999996, 478.34291271000006, 509.44765577, 422.7316338499999, 433.61201734, 413.71486029000005, 498.67011199]
tensor([0.0000e+00, 0.0000e+00, 1.0000e+00, 1.4926e-24, 3.6818e-35, 1.1876e-21,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4777e-26])
all weights top-performance [415.1676209165259]
mean [301.5179089176025, 1740.9827656606553, 415.8845101466536, 631.0890269126093, 613.2012066049707, 905.1287841259718, 402.7346400376497, 827.3557416323247, 353.3068394399494, 1444.0416896968084]
