  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:00<00:04,  1.97it/s] 20%|██        | 2/10 [00:01<00:04,  1.68it/s] 30%|███       | 3/10 [00:01<00:03,  1.84it/s] 40%|████      | 4/10 [00:02<00:03,  1.80it/s] 50%|█████     | 5/10 [00:02<00:02,  1.79it/s] 60%|██████    | 6/10 [00:03<00:02,  1.87it/s] 70%|███████   | 7/10 [00:03<00:01,  1.82it/s] 80%|████████  | 8/10 [00:04<00:01,  1.82it/s] 90%|█████████ | 9/10 [00:04<00:00,  1.82it/s]100%|██████████| 10/10 [00:05<00:00,  1.92it/s]100%|██████████| 10/10 [00:05<00:00,  1.85it/s]
/home/mschlichting/RLSoups/average_models.py:70: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  weights = torch.nn.functional.softmax(performance)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  3.61it/s]100%|██████████| 1/1 [00:00<00:00,  3.60it/s]
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:00<00:05,  1.55it/s] 20%|██        | 2/10 [00:01<00:05,  1.57it/s] 30%|███       | 3/10 [00:01<00:04,  1.56it/s] 40%|████      | 4/10 [00:02<00:03,  1.59it/s] 50%|█████     | 5/10 [00:03<00:03,  1.62it/s] 60%|██████    | 6/10 [00:03<00:02,  1.51it/s] 70%|███████   | 7/10 [00:04<00:02,  1.48it/s] 80%|████████  | 8/10 [00:05<00:01,  1.47it/s] 90%|█████████ | 9/10 [00:05<00:00,  1.49it/s]100%|██████████| 10/10 [00:06<00:00,  1.46it/s]100%|██████████| 10/10 [00:06<00:00,  1.51it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.51it/s]100%|██████████| 1/1 [00:00<00:00,  1.51it/s]
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:00<00:08,  1.11it/s] 20%|██        | 2/10 [00:01<00:07,  1.14it/s] 30%|███       | 3/10 [00:02<00:06,  1.16it/s] 40%|████      | 4/10 [00:03<00:05,  1.14it/s] 50%|█████     | 5/10 [00:04<00:04,  1.16it/s] 60%|██████    | 6/10 [00:05<00:03,  1.16it/s] 70%|███████   | 7/10 [00:06<00:02,  1.17it/s] 80%|████████  | 8/10 [00:06<00:01,  1.16it/s] 90%|█████████ | 9/10 [00:07<00:00,  1.09it/s]100%|██████████| 10/10 [00:08<00:00,  1.08it/s]100%|██████████| 10/10 [00:08<00:00,  1.13it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.17it/s]100%|██████████| 1/1 [00:00<00:00,  1.17it/s]
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:00<00:08,  1.04it/s] 20%|██        | 2/10 [00:01<00:07,  1.10it/s] 30%|███       | 3/10 [00:02<00:06,  1.07it/s] 40%|████      | 4/10 [00:03<00:05,  1.03it/s] 50%|█████     | 5/10 [00:04<00:04,  1.05it/s] 60%|██████    | 6/10 [00:05<00:03,  1.05it/s] 70%|███████   | 7/10 [00:06<00:02,  1.04it/s] 80%|████████  | 8/10 [00:07<00:02,  1.03s/it] 90%|█████████ | 9/10 [00:09<00:01,  1.14s/it]100%|██████████| 10/10 [00:10<00:00,  1.12s/it]100%|██████████| 10/10 [00:10<00:00,  1.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.12it/s]100%|██████████| 1/1 [00:00<00:00,  1.12it/s]
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:00<00:08,  1.07it/s] 20%|██        | 2/10 [00:01<00:07,  1.05it/s] 30%|███       | 3/10 [00:02<00:06,  1.04it/s] 40%|████      | 4/10 [00:03<00:05,  1.02it/s] 50%|█████     | 5/10 [00:05<00:05,  1.05s/it] 60%|██████    | 6/10 [00:06<00:04,  1.05s/it] 70%|███████   | 7/10 [00:07<00:03,  1.04s/it] 80%|████████  | 8/10 [00:08<00:02,  1.04s/it] 90%|█████████ | 9/10 [00:09<00:01,  1.09s/it]100%|██████████| 10/10 [00:10<00:00,  1.08s/it]100%|██████████| 10/10 [00:10<00:00,  1.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.09s/it]100%|██████████| 1/1 [00:01<00:00,  1.09s/it]
0 0
Current Training Reward 164.54596045
0 1
Current Training Reward 182.74999004999998
0 2
Current Training Reward 173.59466068000006
0 3
Current Training Reward 173.32872469000003
0 4
Current Training Reward 165.912352
0 5
Current Training Reward 163.74893551
0 6
Current Training Reward 172.49823433
0 7
Current Training Reward 183.27195609999998
0 8
Current Training Reward 164.65868552999996
0 9
Current Training Reward 172.50696018000002
[192.87516634915545, 244.92371996848988, 179.16137037439785, 209.82348405813747, 209.92763209286028, 185.08150433536568, 212.99491962726924, 212.3534346058744, 210.3958322601703, 166.8290905430302]
[164.54596045, 182.74999004999998, 173.59466068000006, 173.32872469000003, 165.912352, 163.74893551, 172.49823433, 183.27195609999998, 164.65868552999996, 172.50696018000002]
tensor([4.6243e-09, 3.7236e-01, 3.9342e-05, 3.0155e-05, 1.8133e-08, 2.0840e-09,
        1.3142e-05, 6.2755e-01, 5.1762e-09, 1.3258e-05])
all weights top-performance [79.10879868683597]
mean [192.87516634915545, 244.92371996848988, 179.16137037439785, 209.82348405813747, 209.92763209286028, 185.08150433536568, 212.99491962726924, 212.3534346058744, 210.3958322601703, 166.8290905430302]
1 0
Current Training Reward 221.09405761999997
1 1
Current Training Reward 235.82228903000004
1 2
Current Training Reward 231.77826684000004
1 3
Current Training Reward 235.71733886
1 4
Current Training Reward 205.31231468000001
1 5
Current Training Reward 227.0714405
1 6
Current Training Reward 230.3864045
1 7
Current Training Reward 220.54710618000001
1 8
Current Training Reward 213.17948941000003
1 9
Current Training Reward 234.18539581000002
[265.00921986805986, 249.59519331514244, 266.9781695688808, 248.6221177602453, 238.8332584386397, 299.9538969284675, 304.46022472085576, 266.2198553340917, 263.69106042017773, 302.4462703598464]
[221.09405761999997, 235.82228903000004, 231.77826684000004, 235.71733886, 205.31231468000001, 227.0714405, 230.3864045, 220.54710618000001, 213.17948941000003, 234.18539581000002]
tensor([1.8962e-07, 4.7237e-01, 8.2790e-03, 4.2531e-01, 2.6544e-14, 7.4788e-05,
        2.0583e-03, 1.0974e-07, 6.9284e-11, 9.1914e-02])
all weights top-performance [263.70331044360756]
mean [265.00921986805986, 249.59519331514244, 266.9781695688808, 248.6221177602453, 238.8332584386397, 299.9538969284675, 304.46022472085576, 266.2198553340917, 263.69106042017773, 302.4462703598464]
2 0
Current Training Reward 336.46382757
2 1
Current Training Reward 352.15013869
2 2
Current Training Reward 344.27772102
2 3
Current Training Reward 343.44221387000005
2 4
Current Training Reward 369.46059197
2 5
Current Training Reward 348.84950869999994
2 6
Current Training Reward 338.9351406100001
2 7
Current Training Reward 364.71782135
2 8
Current Training Reward 365.68552915
2 9
Current Training Reward 348.19829969
[446.51337627035826, 406.7767774199682, 397.58088615658085, 412.10240780995673, 401.8589310454445, 409.31841682236876, 416.3423876799234, 412.3364844554136, 479.1762566293986, 459.90404403292706]
[336.46382757, 352.15013869, 344.27772102, 343.44221387000005, 369.46059197, 348.84950869999994, 338.9351406100001, 364.71782135, 365.68552915, 348.19829969]
tensor([4.5306e-15, 2.9419e-08, 1.1212e-11, 4.8620e-12, 9.6932e-01, 1.0844e-09,
        5.3633e-14, 8.4471e-03, 2.2232e-02, 5.6542e-10])
all weights top-performance [424.6683002210997]
mean [446.51337627035826, 406.7767774199682, 397.58088615658085, 412.10240780995673, 401.8589310454445, 409.31841682236876, 416.3423876799234, 412.3364844554136, 479.1762566293986, 459.90404403292706]
3 0
Current Training Reward 482.78904403000007
3 1
Current Training Reward 481.89640195999993
3 2
Current Training Reward 475.60616358
3 3
Current Training Reward 462.81712321
3 4
Current Training Reward 469.26967503
3 5
Current Training Reward 458.04804665000006
3 6
Current Training Reward 422.08207404999996
3 7
Current Training Reward 479.5642556499999
3 8
Current Training Reward 478.89330629000006
3 9
Current Training Reward 465.08061302999994
[508.7664612945124, 415.29653891363495, 481.2382679756257, 561.317658293822, 458.72105073478167, 493.0964379056526, 511.1048198352961, 646.5684884137457, 737.4893525691708, 527.2864507176788]
[482.78904403000007, 481.89640195999993, 475.60616358, 462.81712321, 469.26967503, 458.04804665000006, 422.08207404999996, 479.5642556499999, 478.89330629000006, 465.08061302999994]
tensor([6.8007e-01, 2.7854e-01, 5.1651e-04, 1.4417e-09, 9.1449e-07, 1.2237e-11,
        2.9367e-27, 2.7043e-02, 1.3825e-02, 1.3864e-08])
all weights top-performance [458.9284847811793]
mean [508.7664612945124, 415.29653891363495, 481.2382679756257, 561.317658293822, 458.72105073478167, 493.0964379056526, 511.1048198352961, 646.5684884137457, 737.4893525691708, 527.2864507176788]
4 0
Current Training Reward 503.59824760000004
4 1
Current Training Reward 519.71813487
4 2
Current Training Reward 516.82493259
4 3
Current Training Reward 486.87631124
4 4
Current Training Reward 548.76076756
4 5
Current Training Reward 504.24868427
4 6
Current Training Reward 517.90917346
4 7
Current Training Reward 497.5393275300001
4 8
Current Training Reward 526.58114242
4 9
Current Training Reward 535.33629731
[500.53907594926824, 529.6147038302654, 498.5197664149823, 560.6954236332384, 606.5851311354185, 560.8124153011673, 531.4654278312015, 518.2851484507456, 665.5273623811183, 571.8280875554772]
[503.59824760000004, 519.71813487, 516.82493259, 486.87631124, 548.76076756, 504.24868427, 517.90917346, 497.5393275300001, 526.58114242, 535.33629731]
tensor([2.4332e-20, 2.4376e-13, 1.3504e-14, 1.3302e-27, 1.0000e+00, 4.6629e-20,
        3.9933e-14, 5.6862e-23, 2.3308e-10, 1.4786e-06])
all weights top-performance [549.9993930944122]
mean [500.53907594926824, 529.6147038302654, 498.5197664149823, 560.6954236332384, 606.5851311354185, 560.8124153011673, 531.4654278312015, 518.2851484507456, 665.5273623811183, 571.8280875554772]
