  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:01<00:12,  1.37s/it] 20%|██        | 2/10 [00:01<00:06,  1.19it/s] 30%|███       | 3/10 [00:02<00:04,  1.52it/s] 40%|████      | 4/10 [00:03<00:04,  1.24it/s] 50%|█████     | 5/10 [00:03<00:03,  1.58it/s] 60%|██████    | 6/10 [00:04<00:02,  1.74it/s] 70%|███████   | 7/10 [00:04<00:01,  1.53it/s] 80%|████████  | 8/10 [00:06<00:01,  1.25it/s] 90%|█████████ | 9/10 [00:06<00:00,  1.35it/s]100%|██████████| 10/10 [00:07<00:00,  1.11it/s]100%|██████████| 10/10 [00:07<00:00,  1.27it/s]
/home/mschlichting/RLSoups/average_models.py:70: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  weights = torch.nn.functional.softmax(performance)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  1.27it/s]100%|██████████| 1/1 [00:00<00:00,  1.27it/s]
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:00<00:08,  1.04it/s] 20%|██        | 2/10 [00:01<00:07,  1.07it/s] 30%|███       | 3/10 [00:02<00:06,  1.11it/s] 40%|████      | 4/10 [00:03<00:04,  1.28it/s] 50%|█████     | 5/10 [00:04<00:04,  1.19it/s] 60%|██████    | 6/10 [00:05<00:04,  1.13s/it] 70%|███████   | 7/10 [00:07<00:03,  1.13s/it] 80%|████████  | 8/10 [00:07<00:02,  1.06s/it] 90%|█████████ | 9/10 [00:08<00:00,  1.00it/s]100%|██████████| 10/10 [00:09<00:00,  1.01it/s]100%|██████████| 10/10 [00:09<00:00,  1.02it/s]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:01<00:00,  1.73s/it]100%|██████████| 1/1 [00:01<00:00,  1.73s/it]
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:01<00:13,  1.49s/it] 20%|██        | 2/10 [00:06<00:26,  3.29s/it] 30%|███       | 3/10 [00:09<00:24,  3.50s/it] 40%|████      | 4/10 [00:11<00:15,  2.62s/it] 50%|█████     | 5/10 [00:13<00:12,  2.41s/it] 60%|██████    | 6/10 [00:16<00:11,  2.88s/it] 70%|███████   | 7/10 [00:19<00:08,  2.81s/it] 80%|████████  | 8/10 [00:21<00:04,  2.47s/it] 90%|█████████ | 9/10 [00:23<00:02,  2.33s/it]100%|██████████| 10/10 [00:25<00:00,  2.37s/it]100%|██████████| 10/10 [00:25<00:00,  2.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:04<00:00,  4.44s/it]100%|██████████| 1/1 [00:04<00:00,  4.44s/it]
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:04<00:39,  4.34s/it] 20%|██        | 2/10 [00:05<00:20,  2.52s/it] 30%|███       | 3/10 [00:09<00:22,  3.21s/it] 40%|████      | 4/10 [00:13<00:20,  3.50s/it] 50%|█████     | 5/10 [00:15<00:13,  2.80s/it] 60%|██████    | 6/10 [00:18<00:12,  3.09s/it] 70%|███████   | 7/10 [00:21<00:08,  2.94s/it] 80%|████████  | 8/10 [00:22<00:04,  2.43s/it] 90%|█████████ | 9/10 [00:24<00:02,  2.19s/it]100%|██████████| 10/10 [00:27<00:00,  2.55s/it]100%|██████████| 10/10 [00:27<00:00,  2.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:04<00:00,  4.26s/it]100%|██████████| 1/1 [00:04<00:00,  4.26s/it]
  0%|          | 0/10 [00:00<?, ?it/s] 10%|█         | 1/10 [00:04<00:42,  4.67s/it] 20%|██        | 2/10 [00:08<00:32,  4.05s/it] 30%|███       | 3/10 [00:12<00:28,  4.07s/it] 40%|████      | 4/10 [00:14<00:19,  3.30s/it] 50%|█████     | 5/10 [00:19<00:18,  3.75s/it] 60%|██████    | 6/10 [00:22<00:14,  3.56s/it] 70%|███████   | 7/10 [00:26<00:10,  3.67s/it] 80%|████████  | 8/10 [00:30<00:07,  3.86s/it] 90%|█████████ | 9/10 [00:34<00:04,  4.06s/it]100%|██████████| 10/10 [00:39<00:00,  4.21s/it]100%|██████████| 10/10 [00:39<00:00,  3.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:03<00:00,  3.87s/it]100%|██████████| 1/1 [00:03<00:00,  3.87s/it]
0 0
Current Training Reward 167.00371063
0 1
Current Training Reward 203.48892623000003
0 2
Current Training Reward 107.94831759
0 3
Current Training Reward 84.73503328000001
0 4
Current Training Reward 72.45858042
0 5
Current Training Reward 208.16539423999998
0 6
Current Training Reward 259.53065724
0 7
Current Training Reward 209.12184449999998
0 8
Current Training Reward 221.28346539000003
0 9
Current Training Reward 168.95197025999997
[205.75025544781744, 107.49788982654954, 175.67235483164004, 87.16152686063478, 71.18646204087688, 142.60190375900282, 330.70686738707366, 306.888667094442, 240.93851318626974, 344.33098540589737]
[167.00371063, 203.48892623000003, 107.94831759, 84.73503328000001, 72.45858042, 208.16539423999998, 259.53065724, 209.12184449999998, 221.28346539000003, 168.95197025999997]
tensor([6.5471e-41, 4.5854e-25, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.9243e-23,
        1.0000e+00, 1.2815e-22, 2.4516e-17, 4.5938e-40])
all weights top-performance [321.03559604678594]
mean [205.75025544781744, 107.49788982654954, 175.67235483164004, 87.16152686063478, 71.18646204087688, 142.60190375900282, 330.70686738707366, 306.888667094442, 240.93851318626974, 344.33098540589737]
1 0
Current Training Reward 360.71972085
1 1
Current Training Reward 393.14745862
1 2
Current Training Reward 335.41217575
1 3
Current Training Reward 332.0329245257732
1 4
Current Training Reward 357.8180769789472
1 5
Current Training Reward 427.69971293750007
1 6
Current Training Reward 281.02194389
1 7
Current Training Reward 352.13405694
1 8
Current Training Reward 322.51656102
1 9
Current Training Reward 380.8488775698924
[423.5001845952999, 435.2167961520689, 459.7082916341331, 269.618767709943, 396.3834153475217, 918.3032353712266, 471.1377711424623, 311.2558838748732, 344.94930986538554, 411.5546493186471]
[360.71972085, 393.14745862, 335.41217575, 332.0329245257732, 357.8180769789472, 427.69971293750007, 281.02194389, 352.13405694, 322.51656102, 380.8488775698924]
tensor([8.1464e-30, 9.8662e-16, 8.3182e-41, 2.8348e-42, 4.4750e-31, 1.0000e+00,
        0.0000e+00, 1.5215e-33, 0.0000e+00, 4.4972e-21])
all weights top-performance [943.4012857808772]
mean [423.5001845952999, 435.2167961520689, 459.7082916341331, 269.618767709943, 396.3834153475217, 918.3032353712266, 471.1377711424623, 311.2558838748732, 344.94930986538554, 411.5546493186471]
2 0
Current Training Reward 676.4266171212121
2 1
Current Training Reward 1385.3257563749999
2 2
Current Training Reward 596.2682234492754
2 3
Current Training Reward 661.5714969189189
2 4
Current Training Reward 823.0612659818183
2 5
Current Training Reward 597.8879852419353
2 6
Current Training Reward 594.21546148
2 7
Current Training Reward 745.9868516428571
2 8
Current Training Reward 669.9382213188405
2 9
Current Training Reward 750.8323771846153
[713.1636795915858, 2480.279051463728, 1295.0927427982406, 678.2800978441643, 1078.150101478358, 1399.1850598815095, 1069.7924373534001, 592.8947349036738, 1058.9239168463694, 1282.7706414446216]
[676.4266171212121, 1385.3257563749999, 596.2682234492754, 661.5714969189189, 823.0612659818183, 597.8879852419353, 594.21546148, 745.9868516428571, 669.9382213188405, 750.8323771846153]
tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])
all weights top-performance [2482.9763375628927]
mean [713.1636795915858, 2480.279051463728, 1295.0927427982406, 678.2800978441643, 1078.150101478358, 1399.1850598815095, 1069.7924373534001, 592.8947349036738, 1058.9239168463694, 1282.7706414446216]
3 0
Current Training Reward 1582.0408535588235
3 1
Current Training Reward 1284.6234483023256
3 2
Current Training Reward 1216.1733138181817
3 3
Current Training Reward 1547.5517873333333
3 4
Current Training Reward 758.489205262295
3 5
Current Training Reward 1096.2336874489797
3 6
Current Training Reward 1283.6372285500001
3 7
Current Training Reward 1010.213982925926
3 8
Current Training Reward 1029.1841701836736
3 9
Current Training Reward 1186.4727758636363
[2869.3665156944157, 758.1917023814877, 2448.8719502932263, 2458.3956504295156, 829.6418097724711, 1990.8450900715095, 1730.9660370147835, 716.7551943938469, 1051.9164736981318, 2143.776867802355]
[1582.0408535588235, 1284.6234483023256, 1216.1733138181817, 1547.5517873333333, 758.489205262295, 1096.2336874489797, 1283.6372285500001, 1010.213982925926, 1029.1841701836736, 1186.4727758636363]
tensor([1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0509e-15, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00])
all weights top-performance [2854.6788878953316]
mean [2869.3665156944157, 758.1917023814877, 2448.8719502932263, 2458.3956504295156, 829.6418097724711, 1990.8450900715095, 1730.9660370147835, 716.7551943938469, 1051.9164736981318, 2143.776867802355]
4 0
Current Training Reward 1589.80985775
4 1
Current Training Reward 1315.8400297142857
4 2
Current Training Reward 2299.384378740741
4 3
Current Training Reward 2039.6883429655174
4 4
Current Training Reward 1662.450276277778
4 5
Current Training Reward 1093.786198425532
4 6
Current Training Reward 2667.8385544166663
4 7
Current Training Reward 1916.87147753125
4 8
Current Training Reward 1881.66718459375
4 9
Current Training Reward 1165.9774320000001
[3028.487816334994, 2014.940246409495, 2829.415033146219, 1477.236321084853, 2834.3939427177975, 1562.74846515934, 2786.2526980972348, 3119.9535124689583, 3018.414806768557, 3074.7579402426813]
[1589.80985775, 1315.8400297142857, 2299.384378740741, 2039.6883429655174, 1662.450276277778, 1093.786198425532, 2667.8385544166663, 1916.87147753125, 1881.66718459375, 1165.9774320000001]
tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])
all weights top-performance [2756.4561663563127]
mean [3028.487816334994, 2014.940246409495, 2829.415033146219, 1477.236321084853, 2834.3939427177975, 1562.74846515934, 2786.2526980972348, 3119.9535124689583, 3018.414806768557, 3074.7579402426813]
